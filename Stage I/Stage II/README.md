# **BOLD-Cast**: Modeling individual-level long-range brain dynamics from short fMRI scans
<!-- <i>The avatar is generated by DALLE-3.</i> -->


This repository is the official implementation of the paper **Modeling individual-level long-range brain dynamics from short fMRI scans**.


## ðŸš€Overview
We present an autoregressive model for Blood Oxygen Level Dependent (BOLD) fMRI time series forecasting (BOLD-Cast). The core strength of BOLD-Cast is the explicit joint learning of both cohort-invariant and subject-specific functional topology to learn individualized prediction with explicit temporal anchoring. We conducted primary experiments on UK Biobank (UKB) dataset of 36,228 subjects. The results demonstrate that BOLD-Cast achieves the lowest prediction error and highest temporal correlation. Subsequently, we validated it across 4 external datasets, spanning ages from adolescents to the elderly and diverse clinical conditions. We obtained reliable zero-shot generalization without the need for retraining. We additionally obtained temporal scalability, generating predictions of arbitrary length to flexibly meet the specific demands of diverse clinical applications. Critically, we show that extended fMRI signals using BOLD-Cast consistently improves performance in autism classification and sex identification tasks. Such improvements are particularly beneficial in populations where long acquisitions are difficult (e.g., children, the elderly, or cohorts with neuropsychiatric conditions), without increasing scan duration.

## ðŸ“ŠUsage 

**Preparation**

Install Pytorch and necessary dependencies.

```
pip install -r requirements.txt
```

**For Stage I**
1. Put the datasets under the folder ```./dataset/```.

2. Important args:
* `--use_pretrain` Test checkpoints in [checkpoints](checkpoints)
* `--dataset` ukb hcp-d hcp-ya hcp-a abide
* `--custom_key` Node: node classification
3. Training
* `python prepare_data.py 
* `python main.py

4. Testing
* `use_pretrain == 'True'

**For Stage II**
1. Put the datasets under the folder ```./dataset/```.
2. Download the large language models from [Hugging Face](https://huggingface.co/). The default LLM is LLaMA-7B, you can change the `llm_ckp_dir` in `run.py` to use [GPT2](https://huggingface.co/openai-community/gpt2)

   For example, if you download and put the LLaMA directory successfully, the directory structure is as follows:
   - data_provider
   - dataset
   - gpt2
     - config.json
     - flax_model.msgpack
     - generation_config.json
     - ...
   - ...
   - run.py
3. Generate timestamp by gpt2, suffixed by {subjectid}.pt, \
python [generate_timestamp_ukb.py](generate%20timestamp%2Fgenerate_timestamp_ukb.py) 
4. Generate timestamp embeddings, and save them along with historical time series, common features, and characteristic features as H5 data, stored in dataset/ukb_input. The training, value, and test datasets need to be generated in three separate batches. 
python [preprocess_ts.py](preprocess_ts.py)
5. Training
python [run.py](run.py)
6. Testing
* `use_pretrain == 'True'


## âœ¨Comparison
The comparison algorithm has been integrated into the models. Some GNN-based models cannot be integrated and are therefore not included.


## Acknowledgement
We appreciate the following GitHub repos a lot for their valuable code and efforts.
- Time-Series-Library (https://github.com/thuml/Time-Series-Library)
- FPT (https://github.com/DAMO-DI-ML/NeurIPS2023-One-Fits-All)